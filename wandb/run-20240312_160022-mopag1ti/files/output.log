













  0%|▍                                                                                                                   | 13/4000 [02:41<12:42:54, 11.48s/it]


















  1%|▉                                                                                                                   | 31/4000 [06:06<12:19:28, 11.18s/it]



  1%|▉                                                                                                                   | 34/4000 [06:40<12:29:09, 11.33s/it]












  1%|█▎                                                                                                                  | 46/4000 [08:54<12:06:51, 11.03s/it]






















































  2%|██▉                                                                                                                | 100/4000 [19:06<12:10:02, 11.23s/it]



































































































  5%|█████▋                                                                                                             | 199/4000 [37:38<11:46:30, 11.15s/it]




































































































  7%|████████▌                                                                                                          | 299/4000 [56:29<11:33:01, 11.24s/it]




































































































 10%|███████████▎                                                                                                     | 399/4000 [1:15:13<11:13:55, 11.23s/it]




































































































 12%|██████████████                                                                                                   | 499/4000 [1:34:07<11:07:20, 11.44s/it]
 12%|██████████████▏                                                                                                  | 500/4000 [1:34:19<11:03:10, 11.37s/it][INFO|tokenization_utils_base.py:2171] 2024-03-12 17:34:48,204 >> tokenizer config file saved in saved/moelora/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-12 17:34:48,265 >> Special tokens file saved in saved/moelora/checkpoint-500/special_tokens_map.json
/cpfs01/user/chenqin.p/anaconda3/envs/moe/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-12 17:34:48,327] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2024-03-12 17:34:52,027] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2024-03-12 17:34:52,027] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-500/global_step500/mp_rank_00_model_states.pt...
[2024-03-12 17:35:13,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-500/global_step500/mp_rank_00_model_states.pt.
[2024-03-12 17:35:13,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-12 17:35:14,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-12 17:35:14,347] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-12 17:35:14,347] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!


































































































 15%|████████████████▉                                                                                                | 599/4000 [1:53:29<10:47:09, 11.42s/it]





































































































 18%|███████████████████▊                                                                                             | 700/4000 [2:12:38<10:25:14, 11.37s/it]



































































































 20%|██████████████████████▌                                                                                          | 799/4000 [2:31:31<10:09:28, 11.42s/it]




































































































 22%|█████████████████████████▌                                                                                        | 899/4000 [2:50:33<9:45:56, 11.34s/it]




































































































 25%|████████████████████████████▍                                                                                     | 999/4000 [3:09:34<9:28:08, 11.36s/it]

 25%|████████████████████████████▎                                                                                    | 1000/4000 [3:09:45<9:22:24, 11.25s/it]
 25%|████████████████████████████▎                                                                                    | 1000/4000 [3:09:45<9:22:24, 11.25s/it][INFO|tokenization_utils_base.py:2171] 2024-03-12 19:10:15,472 >> tokenizer config file saved in saved/moelora/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-12 19:10:15,472 >> Special tokens file saved in saved/moelora/checkpoint-1000/special_tokens_map.json
[2024-03-12 19:10:18,945] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2024-03-12 19:10:18,945] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2024-03-12 19:10:40,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2024-03-12 19:10:40,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-12 19:10:41,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-12 19:10:41,362] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-12 19:10:41,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!

































































































 27%|███████████████████████████████                                                                                  | 1099/4000 [3:28:58<9:15:28, 11.49s/it]




































































































 30%|█████████████████████████████████▊                                                                               | 1199/4000 [3:47:54<8:54:19, 11.45s/it]





































































































 32%|████████████████████████████████████▋                                                                            | 1300/4000 [4:07:01<8:29:43, 11.33s/it]




































































































 35%|███████████████████████████████████████▌                                                                         | 1400/4000 [4:25:51<8:11:07, 11.33s/it]




































































































 38%|██████████████████████████████████████████▍                                                                      | 1500/4000 [4:44:33<7:44:43, 11.15s/it]
{'loss': 0.5901, 'learning_rate': 0.00012531265632816408, 'epoch': 1.5}
 38%|██████████████████████████████████████████▍                                                                      | 1500/4000 [4:44:33<7:44:43, 11.15s/it][INFO|tokenization_utils_base.py:2171] 2024-03-12 20:45:03,063 >> tokenizer config file saved in saved/moelora/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-12 20:45:03,064 >> Special tokens file saved in saved/moelora/checkpoint-1500/special_tokens_map.json
[2024-03-12 20:45:06,760] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt
[2024-03-12 20:45:06,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt...
[2024-03-12 20:45:28,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt.
[2024-03-12 20:45:28,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-12 20:45:29,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-12 20:45:29,217] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-12 20:45:29,217] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!


































































































 40%|█████████████████████████████████████████████▏                                                                   | 1599/4000 [5:03:21<7:26:36, 11.16s/it]




































































































 42%|███████████████████████████████████████████████▉                                                                 | 1699/4000 [5:21:45<7:00:06, 10.95s/it]




































































































 45%|██████████████████████████████████████████████████▊                                                              | 1799/4000 [5:39:58<6:37:25, 10.83s/it]





































































































 48%|█████████████████████████████████████████████████████▋                                                           | 1900/4000 [5:58:26<6:24:49, 10.99s/it]




































































































 50%|████████████████████████████████████████████████████████▌                                                        | 2000/4000 [6:16:46<6:04:42, 10.94s/it]
[2024-03-12 22:17:14,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=4, lr=[0.00010030015007503752], mom=[[0.9, 0.999]]
[2024-03-12 22:17:14,411] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=2000, RunningAvgSamplesPerSec=2.912874383053494, CurrSamplesPerSec=3.0171323732701465, MemAllocated=13.38GB, MaxMemAllocated=47.81GB
 50%|████████████████████████████████████████████████████████▌                                                        | 2000/4000 [6:16:46<6:04:42, 10.94s/it][INFO|tokenization_utils_base.py:2171] 2024-03-12 22:17:15,933 >> tokenizer config file saved in saved/moelora/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-12 22:17:15,934 >> Special tokens file saved in saved/moelora/checkpoint-2000/special_tokens_map.json
[2024-03-12 22:17:15,984] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[2024-03-12 22:17:19,532] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt
[2024-03-12 22:17:19,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...
[2024-03-12 22:17:41,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.
[2024-03-12 22:17:41,302] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-12 22:17:45,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-12 22:17:45,098] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-12 22:17:45,098] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!



































































































 52%|███████████████████████████████████████████████████████████▎                                                     | 2100/4000 [6:35:23<5:43:13, 10.84s/it]




































































































 55%|██████████████████████████████████████████████████████████████▏                                                  | 2200/4000 [6:53:26<5:22:36, 10.75s/it]



































































































 57%|████████████████████████████████████████████████████████████████▉                                                | 2299/4000 [7:11:15<5:05:39, 10.78s/it]





































































































 60%|███████████████████████████████████████████████████████████████████▊                                             | 2400/4000 [7:29:27<4:46:42, 10.75s/it]



































































































 62%|██████████████████████████████████████████████████████████████████████▌                                          | 2499/4000 [7:47:14<4:29:52, 10.79s/it]
 62%|██████████████████████████████████████████████████████████████████████▋                                          | 2500/4000 [7:47:26<4:31:38, 10.87s/it][INFO|tokenization_utils_base.py:2171] 2024-03-12 23:47:55,439 >> tokenizer config file saved in saved/moelora/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-12 23:47:55,440 >> Special tokens file saved in saved/moelora/checkpoint-2500/special_tokens_map.json
[2024-03-12 23:47:55,490] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2500 is about to be saved!
[2024-03-12 23:47:59,015] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt
[2024-03-12 23:47:59,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt...
[2024-03-12 23:48:20,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt.
[2024-03-12 23:48:20,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-12 23:48:21,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-12 23:48:21,360] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-12 23:48:21,360] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!



































































































 65%|█████████████████████████████████████████████████████████████████████████▍                                       | 2600/4000 [8:05:47<4:11:41, 10.79s/it]



































































































 67%|████████████████████████████████████████████████████████████████████████████▏                                    | 2699/4000 [8:23:31<3:52:11, 10.71s/it]





































































































 70%|███████████████████████████████████████████████████████████████████████████████                                  | 2800/4000 [8:41:39<3:36:21, 10.82s/it]




















 70%|███████████████████████████████████████████████████████████████████████████████▋                                 | 2820/4000 [8:45:13<3:28:44, 10.61s/it]















































































 72%|█████████████████████████████████████████████████████████████████████████████████▉                               | 2899/4000 [8:59:24<3:17:15, 10.75s/it]




































































































 75%|████████████████████████████████████████████████████████████████████████████████████▋                            | 2999/4000 [9:17:23<2:59:07, 10.74s/it]

 75%|████████████████████████████████████████████████████████████████████████████████████▊                            | 3000/4000 [9:17:34<2:58:58, 10.74s/it]
 75%|████████████████████████████████████████████████████████████████████████████████████▊                            | 3000/4000 [9:17:34<2:58:58, 10.74s/it][INFO|tokenization_utils_base.py:2171] 2024-03-13 01:18:04,269 >> tokenizer config file saved in saved/moelora/checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-13 01:18:04,270 >> Special tokens file saved in saved/moelora/checkpoint-3000/special_tokens_map.json
[2024-03-13 01:18:07,787] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt
[2024-03-13 01:18:07,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt...
[2024-03-13 01:18:29,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt.
[2024-03-13 01:18:29,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-13 01:18:30,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-13 01:18:30,828] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-13 01:18:30,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!


































































































 77%|███████████████████████████████████████████████████████████████████████████████████████▌                         | 3099/4000 [9:35:45<2:40:59, 10.72s/it]





































































































 80%|██████████████████████████████████████████████████████████████████████████████████████████▍                      | 3200/4000 [9:53:52<2:23:17, 10.75s/it]



































































































 82%|████████████████████████████████████████████████████████████████████████████████████████████▎                   | 3299/4000 [10:11:36<2:06:47, 10.85s/it]



























 83%|█████████████████████████████████████████████████████████████████████████████████████████████▏                  | 3326/4000 [10:16:26<1:59:48, 10.66s/it]



 83%|█████████████████████████████████████████████████████████████████████████████████████████████▏                  | 3329/4000 [10:16:58<1:58:16, 10.58s/it]






































































 85%|███████████████████████████████████████████████████████████████████████████████████████████████▏                | 3399/4000 [10:29:28<1:47:00, 10.68s/it]




































































































 88%|██████████████████████████████████████████████████████████████████████████████████████████████████              | 3500/4000 [10:47:33<1:29:43, 10.77s/it][INFO|tokenization_utils_base.py:2171] 2024-03-13 02:48:03,149 >> tokenizer config file saved in saved/moelora/checkpoint-3500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2178] 2024-03-13 02:48:03,150 >> Special tokens file saved in saved/moelora/checkpoint-3500/special_tokens_map.json
{'loss': 0.3785, 'learning_rate': 2.5412706353176592e-05, 'epoch': 3.5}
[2024-03-13 02:48:03,196] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3500 is about to be saved!
[2024-03-13 02:48:06,753] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-3500/global_step3500/mp_rank_00_model_states.pt
[2024-03-13 02:48:06,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-3500/global_step3500/mp_rank_00_model_states.pt...
[2024-03-13 02:48:31,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-3500/global_step3500/mp_rank_00_model_states.pt.
[2024-03-13 02:48:31,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-3500/global_step3500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-13 02:48:32,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-3500/global_step3500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-13 02:48:32,201] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-3500/global_step3500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-13 02:48:32,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3500 is ready now!



































































































 90%|████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 3600/4000 [11:05:59<1:11:45, 10.76s/it]





























 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 3629/4000 [11:11:14<1:06:33, 10.76s/it]







































































 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 3700/4000 [11:24:00<53:53, 10.78s/it]




































































































 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎     | 3800/4000 [11:41:58<35:53, 10.77s/it]



































































































 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 3899/4000 [11:59:43<18:03, 10.72s/it]





































































































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [12:17:54<00:00, 10.76s/it][INFO|tokenization_utils_base.py:2171] 2024-03-13 04:18:23,883 >> tokenizer config file saved in saved/moelora/checkpoint-4000/tokenizer_config.json
[2024-03-13 04:18:22,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=8, lr=[4.502251125562781e-07], mom=[[0.9, 0.999]]
[2024-03-13 04:18:22,865] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=4000, RunningAvgSamplesPerSec=2.9773597673027212, CurrSamplesPerSec=3.095459717221209, MemAllocated=13.38GB, MaxMemAllocated=47.81GB
{'loss': 0.3627, 'learning_rate': 4.502251125562781e-07, 'epoch': 4.0}
[2024-03-13 04:18:23,930] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
[INFO|tokenization_utils_base.py:2178] 2024-03-13 04:18:23,885 >> Special tokens file saved in saved/moelora/checkpoint-4000/special_tokens_map.json
[2024-03-13 04:18:27,424] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saved/moelora/checkpoint-4000/global_step4000/mp_rank_00_model_states.pt
[2024-03-13 04:18:27,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-4000/global_step4000/mp_rank_00_model_states.pt...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [12:18:19<00:00, 11.07s/it]
[2024-03-13 04:18:46,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-4000/global_step4000/mp_rank_00_model_states.pt.
[2024-03-13 04:18:46,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saved/moelora/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-13 04:18:47,632] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saved/moelora/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-13 04:18:47,635] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved saved/moelora/checkpoint-4000/global_step4000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-13 04:18:47,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
{'train_runtime': 44310.6396, 'train_samples_per_second': 2.889, 'train_steps_per_second': 0.09, 'train_loss': 0.5558965110778809, 'epoch': 4.0}
***** train metrics *****
  epoch                    =         4.0
  train_loss               =      0.5559
  train_runtime            = 12:18:30.63
  train_samples            =       32000
  train_samples_per_second =       2.889
  train_steps_per_second   =        0.09